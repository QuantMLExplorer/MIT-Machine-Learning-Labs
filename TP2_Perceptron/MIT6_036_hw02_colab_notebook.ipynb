{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A"
   },
   "source": [
    "#MIT 6.036 Spring 2019: Homework 2#\n",
    "\n",
    "This colab notebook provides code and a framework for problems 7-10 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week2/week2_homework/1).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions (such as `positive`).\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2YM-_zLf9Bp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing code_for_hw02\n",
      "New procedures added: tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv,\n",
      "                      rv, y, positive, score\n",
      "Data Sets: super_simple_separable_through_origin(), super_simple_separable(), xor(),\n",
      "           xor_more()\n",
      "Test data for problem 2.1: data1, labels1, data2, labels2\n",
      "Test data for problem 2.2: big_data, big_data_labels, gen_big_data(), gen_lin_separable(),\n",
      "                           big_higher_dim_separable(), gen_flipped_lin_separable()\n",
      "Test functions: test_linear_classifier(), test_perceptron(), test_averaged_perceptron(),\n",
      "                test_eval_classifier(), test_eval_learning_alg(), test_xval_learning_alg()\n",
      "\n",
      "For more information, use 'help', e.g. 'help tidy_plot'\n",
      "Done with import of code_for_hw02\n"
     ]
    }
   ],
   "source": [
    "!rm -f code_for_hw02.py*\n",
    "!wget --no-check-certificate --quiet https://introml_oll.odl.mit.edu/6.036/static/homework/hw02/code_for_hw02.py\n",
    "from code_for_hw02 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2z1zuhqltjBy",
    "outputId": "d2494cba-852c-4f8c-b129-0320d5d4d10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tidy_plot in module code_for_hw02:\n",
      "\n",
      "tidy_plot(xmin, xmax, ymin, ymax, center=False, title=None, xlabel=None, ylabel=None)\n",
      "    Set up axes for plotting\n",
      "    xmin, xmax, ymin, ymax = (float) plot extents\n",
      "    Return matplotlib axes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tidy_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fdu6T1KpDohy"
   },
   "outputs": [],
   "source": [
    "def test(a):\n",
    "  return a + 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T_soC87TDo69"
   },
   "outputs": [],
   "source": [
    "def methodB(a):\n",
    "  return test(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xnpAY-d_D4J5"
   },
   "outputs": [],
   "source": [
    "def someMethod():\n",
    "  test = 7\n",
    "  return methodB(test + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5_h5O0ND7Fq",
    "outputId": "09c32f7a-aa02-4c75-e88e-b054ff3536d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "someMethod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bhI4dQB1-UZ"
   },
   "source": [
    "# <section>**7) Implement perceptron**</section>\n",
    "\n",
    "Implement [the perceptron algorithm](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week2/perceptron/2), where\n",
    "\n",
    "* `data` is a numpy array of dimension $d$ by $n$\n",
    "* `labels` is numpy array of dimension $1$ by $n$\n",
    "* `params` is a dictionary specifying extra parameters to this algorithm; your algorithm should run a number of iterations equal to $T$\n",
    "* `hook` is either None or a function that takes the tuple `(th, th0)` as an argument and displays the separator graphically.  We won't be testing this in the Tutor, but it will help you in debugging on your own machine.\n",
    "\n",
    "It should return a tuple of $\\theta$ (a $d$ by 1 array) and $\\theta_0$ (a 1 by 1 array).\n",
    "\n",
    "We have given you some  data sets in the code file for you to test your implementation.\n",
    "\n",
    "Your function should initialize all parameters to 0, then run through the data, in the order it is given, performing an update to the parameters whenever the current parameters would make a mistake on that data point. Perform $T$ iterations through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1VOOY4DR-O-q"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VtYf8ysk-VQU"
   },
   "outputs": [],
   "source": [
    "def perceptron(data, labels, params={}, hook=None):\n",
    "    T = params.get('T', 100)\n",
    "    # Your implementation here\n",
    "    nb_points = data.shape[1] #nombre de colonnes\n",
    "    nb_features = data.shape[0]\n",
    "    theta = [0 for j in range(nb_features)]\n",
    "    th = cv(theta)\n",
    "    th0 = cv([0])\n",
    "    for iteration in range(T):\n",
    "        error_made = False\n",
    "        for i in range(nb_points):\n",
    "          x = data[:,i:i+1]\n",
    "          y = labels[0][i]\n",
    "          if (y*positive(x,th,th0) <= 0):\n",
    "              th = th + y*x\n",
    "              th0 = th0 + y\n",
    "              error_made = True\n",
    "        if error_made == False :\n",
    "            return (th, th0)\n",
    "    return (th,th0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92r2oL42-yfM",
    "outputId": "613a97d1-d7b9-4794-e88e-d3e588acf0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Perceptron 0-----------\n",
      "Passed! \n",
      "\n",
      "-----------Test Perceptron 1-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_perceptron(perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QofIY6csIeiW",
    "outputId": "9c3508fc-b29f-4a8b-f4d5-c2f20d8bdaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "theta_0 = np.zeros(1)\n",
    "print(np.shape(theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQMcSWlmB4-Y"
   },
   "source": [
    "# <section>8) Implement averaged perceptron</section>\n",
    "\n",
    "Regular perceptron can be somewhat sensitive to the most recent examples that it sees. Instead, averaged perceptron produces a more stable output by outputting the average value of `th` and `th0` across all iterations.\n",
    "\n",
    "Implement averaged perceptron with the same spec as regular perceptron, and using the pseudocode below as a guide.\n",
    "<pre>\n",
    "procedure averaged_perceptron({(x^(i), y^(i)), i=1,...n}, T)\n",
    "    th = 0 (d by 1); th0 = 0 (1 by 1)\n",
    "    ths = 0 (d by 1); th0s = 0 (1 by 1)\n",
    "    for t = 1,...,T do:\n",
    "        for i = 1,...,n do:\n",
    "\t        if y^(i)(th . x^(i) + th0) <= 0 then\n",
    "              th = th + y^(i)x^(i)\n",
    "              th0 = th0 + y^(i)\n",
    "\t        ths = ths + th\n",
    "\t        th0s = th0s + th0\n",
    "    return ths/(nT), th0s/(nT)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XAwW00MU_FzS"
   },
   "outputs": [],
   "source": [
    "def averaged_perceptron(data, labels, params={}, hook=None):\n",
    "    # if T not in params, default to 100\n",
    "    T = params.get('T', 100)\n",
    "    # Your implementation here\n",
    "    d, n = data.shape\n",
    "    theta = np.zeros((d,1))\n",
    "    theta_0 = np.zeros(1)\n",
    "\n",
    "    thetas = np.zeros((d,1))\n",
    "    theta_0s = np.zeros(1)\n",
    "\n",
    "    for t in range(T):\n",
    "      for i in range(n):\n",
    "        y = labels[0,i]\n",
    "        x = data[:,i]\n",
    "\n",
    "        a = np.dot(x,theta)+theta_0\n",
    "\n",
    "        if np.sign(y*a) <=0: # update the thetas\n",
    "          theta[:,0] = theta[:,0]+ y*x\n",
    "          theta_0 = theta_0 + y\n",
    "\n",
    "        thetas =  thetas + theta\n",
    "        theta_0s = theta_0s + theta_0\n",
    "\n",
    "    return (thetas/(n*T),theta_0s/(n*T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kyLGH0_cBFSU",
    "outputId": "daf23ae8-d7bb-4f9c-c42a-bf10415bc5ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Averaged Perceptron 0-----------\n",
      "Passed! \n",
      "\n",
      "-----------Test Averaged Perceptron 1-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_averaged_perceptron(averaged_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTfGq7LNGceQ"
   },
   "source": [
    "# 9) Implement evaluation strategies\n",
    "  \n",
    "## 9.1)  Evaluating a classifier\n",
    "\n",
    "To evaluate a classifier, we are interested in how well it performs on data that it wasn't trained on. Construct a testing procedure that uses a training data set, calls a learning algorithm to get a linear separator (a tuple of $\\theta, \\theta_0$), and then reports the percentage correct on a new testing set as a float between 0. and 1..\n",
    "\n",
    "The learning algorithm is passed as a function that takes a data array and a labels vector.  Your evaluator should be able to interchangeably evaluate `perceptron` or `averaged_perceptron` (or future algorithms with the same spec), depending on what is passed through the `learner` parameter.\n",
    "\n",
    "The `eval_classifier` function should accept the following parameters:\n",
    "\n",
    "* <tt>learner</tt> - a function, such as perceptron or averaged_perceptron\n",
    "* <tt>data_train</tt> - training data\n",
    "* <tt>labels_train</tt> - training labels\n",
    "* <tt>data_test</tt> - test data\n",
    "* <tt>labels_test</tt> - test labels\n",
    "\n",
    "Assume that you have available the function `score` from HW 1, which takes inputs:\n",
    "\n",
    "* <tt>data</tt>: a <tt>d</tt> by <tt>n</tt> array of floats (representing <tt>n</tt> data points in <tt>d</tt> dimensions)\n",
    "* <tt>labels</tt>: a <tt>1</tt> by <tt>n</tt> array of elements in <tt>(+1, -1)</tt>, representing target labels\n",
    "* <tt>th</tt>: a <tt>d</tt> by <tt>1</tt> array of floats that together with\n",
    "* <tt>th0</tt>: a single scalar or 1 by 1 array, represents a hyperplane\n",
    "\n",
    "and returns 1 by 1 matrix with an integer indicating number of data points correct for the separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uSip1lfHBKaT"
   },
   "outputs": [],
   "source": [
    "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
    "  th, th0 = learner(data_train, labels_train, params={}, hook=None)\n",
    "  nb_correct_points = score(data_test, labels_test, th, th0)\n",
    "  return nb_correct_points / len(labels_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beHMGAb6BTu1",
    "outputId": "e85feea2-fc03-4f19-9785-927f2a56339e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Eval Classifier 0-----------\n",
      "Passed! \n",
      "\n",
      "-----------Test Eval Classifier 1-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_eval_classifier(eval_classifier,perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WPStky3GiJb"
   },
   "source": [
    "## <subsection>9.2) Evaluating a learning algorithm using a data source</subsection>\n",
    "\n",
    "Construct a testing procedure that takes a learning algorithm and a data source as input and runs the learning algorithm multiple times, each time evaluating the resulting classifier as above. It should report the overall average classification accuracy.\n",
    "\n",
    "You can use our implementation of `eval_classifier` as above.\n",
    "\n",
    "Write the function `eval_learning_alg` that takes:\n",
    "\n",
    "* <tt>learner</tt> - a function, such as perceptron or averaged_perceptron\n",
    "* <tt>data_gen</tt> - a data generator, call it with a desired data set size; returns a tuple (data, labels)\n",
    "* <tt>n_train</tt> - the size of the learning sets\n",
    "* <tt>n_test</tt> - the size of the test sets\n",
    "* <tt>it</tt> - the number of iterations to average over\n",
    "\n",
    "and returns the average classification accuracy as a float between 0. and 1..  \n",
    "\n",
    "** Note: Be sure to generate your training data and then testing data in that order, to ensure that the pseudorandomly generated data matches that in the test code. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6qytb8giBXZq"
   },
   "outputs": [],
   "source": [
    "def eval_learning_alg(learner, data_gen, n_train, n_test, it):\n",
    "  avg_accuracy = 0\n",
    "  for i in range(it):\n",
    "    (data_train, labels_train) = data_gen(n_train)\n",
    "    (data_test, labels_test) = data_gen(n_test)\n",
    "    avg_accuracy += eval_classifier(learner, data_train, labels_train, data_test, labels_test)\n",
    "  return avg_accuracy / it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCZojUBJBb06",
    "outputId": "b49cb29f-2c91-4a43-def6-12f092fa34ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Eval Learning Algo-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_eval_learning_alg(eval_learning_alg,perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60u9G0QnGzv-"
   },
   "source": [
    "## <subsection>9.3) Evaluating a learning algorithm with a fixed dataset</subsection>\n",
    "\n",
    "Cross-validation is a strategy for evaluating a learning algorithm, using a single training set of size $n$. Cross-validation takes in a learning algorithm $L$, a fixed data set $\\mathcal{D}$, and a parameter $k$. It will run the learning algorithm $k$ different times, then evaluate the accuracy of the resulting classifier, and ultimately return the average of the accuracies over each of the $k$ \"runs\" of $L$. It is structured like this:\n",
    "\n",
    "<pre><code>divide D into k parts, as equally as possible;  call them D_i for i == 0 .. k-1\n",
    "# be sure the data is shuffled in case someone put all the positive examples first in the data!\n",
    "for j from 0 to k-1:\n",
    "    D_minus_j = union of all the datasets D_i, except for D_j\n",
    "    h_j = L(D_minus_j)\n",
    "    score_j = accuracy of h_j measured on D_j\n",
    "return average(score0, ..., score(k-1))\n",
    "</code></pre>\n",
    "\n",
    "So, each time, it trains on  $k−1$ of the pieces of the data set and tests the resulting hypothesis on the piece that was not used for training.\n",
    "\n",
    "When $k=n$, it is called *leave-one-out cross validation*.\n",
    "\n",
    "Implement cross validation **assuming that the input data is shuffled already** so that the positives and negatives are distributed randomly. If the size of the data does not evenly divide by k, split the data into n % k sub-arrays of size n//k + 1 and the rest of size n//k. (Hint: You can use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_split.html\">numpy.array_split</a>\n",
    "and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html\">numpy.concatenate</a> with axis arguments to split and rejoin the data as you desire.)\n",
    "\n",
    "Note: In Python, n//k indicates integer division, e.g. 2//3 gives 0 and 4//3 gives 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "W5_iixOmBgR7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def xval_learning_alg(learner, data, labels, k):\n",
    "  data_split = np.array_split(data,k,axis=1)\n",
    "  labels_split = np.array_split(labels,k,axis=1)\n",
    "  score = 0\n",
    "  for iteration in range(k):\n",
    "    data_test = data_split[iteration]\n",
    "    labels_test = labels_split[iteration]\n",
    "    data_train = np.concatenate([data_split[i] for i in range(k) if i != iteration],axis=1)\n",
    "    labels_train = np.concatenate([labels_split[i] for i in range(k) if i != iteration],axis=1)\n",
    "    score += eval_classifier(learner,data_train,labels_train,data_test,labels_test)\n",
    "  return score / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUiUgtMHBiZX",
    "outputId": "ff2b82e2-a80c-405d-d40a-3694d9c8f636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Cross-eval Learning Algo-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_xval_learning_alg(xval_learning_alg,perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crF8flfB3hr1"
   },
   "source": [
    "## 10) Testing\n",
    "\n",
    "In this section, we compare the effectiveness of perceptron and averaged perceptron on some data that are not necessarily linearly separable.\n",
    "\n",
    "Use your `eval_learning_alg` and the `gen_flipped_lin_separable` generator in the code file to evaluate the accuracy of `perceptron` vs. a`veraged_perceptron`. `gen_flipped_lin_separable` can be called with an integer to return a data set and labels. Note that this generates linearly separable data and then \"flips\" the labels with some specified probability (the argument pflip); so most of the results will not be linearly separable. You can also specifiy pflip in the call to the generator. You should use the default values of th and th_0 to retain consistency with the Tutor.\n",
    "\n",
    "Run enough trials so that you can confidently predict the accuracy of these algorithms on new data from that same generator; assume training/test sets on the order of 20 points. The Tutor will check that your answer is within 0.025 of the answer we got using the same generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXFoptqiI6Aw",
    "outputId": "33cb6c83-4746-4fec-a4b6-98392583e960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne 0.6\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(30):\n",
    "  sum += eval_learning_alg(perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5)\n",
    "print(\"Moyenne \" + str(sum/30))\n",
    "\n",
    "print(eval_learning_alg(perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYH-JWZvslja",
    "outputId": "d29a39a8-c564-403c-ed14-f7ee2268bf06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne 0.6273333333333334\n",
      "0.5700000000000001\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(30):\n",
    "  sum += eval_learning_alg(averaged_perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5)\n",
    "print(\"Moyenne \" + str(sum/30))\n",
    "\n",
    "print(eval_learning_alg(averaged_perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KNNW9_IXuBO-"
   },
   "outputs": [],
   "source": [
    "# Version modifé de l'algorithme d'évaluation\n",
    "# Data_train = Data_test\n",
    "def eval_learning_alg(learner, data_gen, n_train, n_test, it):\n",
    "  avg_accuracy = 0\n",
    "  for i in range(it):\n",
    "    (data_train, labels_train) = data_gen(n_train)\n",
    "    avg_accuracy += eval_classifier(learner, data_train, labels_train, data_train, labels_train)\n",
    "  return avg_accuracy / it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tv5g4gRDu8ga",
    "outputId": "8d89bc57-6e8a-47dc-84c0-bfe92ba624cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne 0.6533333333333331\n",
      "0.6300000000000001\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(30):\n",
    "  sum += eval_learning_alg(perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5)\n",
    "print(\"Moyenne \" + str(sum/30))\n",
    "\n",
    "print(eval_learning_alg(perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggd_5kR6vG86",
    "outputId": "41509db6-c184-4015-b9de-56996476b76a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne 0.7113333333333335\n",
      "0.7200000000000001\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(30):\n",
    "  sum += eval_learning_alg(averaged_perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5)\n",
    "print(\"Moyenne \" + str(sum/30))\n",
    "\n",
    "print(eval_learning_alg(averaged_perceptron, gen_flipped_lin_separable(pflip=.25), 20, 20, 5))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
